{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if cuda supported GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = pd.read_csv(\n",
    "    '../data/augmented_training_file/final_train_file2.csv')\n",
    "final_test_file = pd.read_csv('../data/final_test_file.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows grouped by label are:\n",
      "0 class rows: 175598 (40.240)% of total train data\n",
      "2 class rows: 112302 (25.735)% of total train data\n",
      "1 class rows: 148476 (34.025)% of total train data\n"
     ]
    }
   ],
   "source": [
    "data_per_class = Counter(train_file['label'])\n",
    "print(\"Number of rows grouped by label are:\")\n",
    "for i, label in enumerate(data_per_class):\n",
    "    print(\n",
    "        f\"{label} class rows: {data_per_class[label]} ({data_per_class[label] / train_file.shape[0]*100:.3f})% of total train data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights to be used for CrossEntropyLoss (increases loss to classes with less data to balance the learning)\n",
      " tensor([0.8284, 0.9797, 1.2952])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\CVAS4\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0 1 2], y=0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "436371    1\n",
      "436372    1\n",
      "436373    1\n",
      "436374    1\n",
      "436375    1\n",
      "Name: label, Length: 436376, dtype: int64 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight  # .compute_class_weight\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced', np.unique(train_file['label']), train_file['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\n",
    "    f\"Class weights to be used for CrossEntropyLoss (increases loss to classes with less data to balance the learning)\\n {class_weights}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_size = 0.3  # percentage of test split\n",
    "train_split, test_split = train_test_split(\n",
    "    train_file, test_size=test_split_size, stratify=train_file['label'], random_state=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating customised Image dataset to be used for dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_frame):\n",
    "        # print(data_frame['encoded_titles_combined'])\n",
    "        self.data = data_frame\n",
    "        self.label = data_frame['label']\n",
    "        self.length=len(data_frame)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # label_encoding = {\"unrelated\": 0, \"agreed\": 1, \"disagreed\": 2}\n",
    "        # print(type(self.data[idx]), self.data[idx])\n",
    "        # print(self.data['title1_en_encoded'][idx])\n",
    "        # print(self.data['title2_en_encoded'][idx])\n",
    "        title_1 = self.data['title1_en_encoded'][idx]\n",
    "        title_1= np.array(title_1[1:-1].split(), dtype=int)\n",
    "        title_1=torch.tensor(title_1)\n",
    "\n",
    "        title_2 = self.data['title2_en_encoded'][idx]\n",
    "        title_2= np.array(title_2[1:-1].split(), dtype=int)\n",
    "        title_2=torch.tensor(title_2)\n",
    "\n",
    "        label = torch.tensor(self.label[idx]).long()\n",
    "        data=(title_1,title_2,label)\n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_dataset=CustomDataset(train_split.reset_index())\n",
    "test_split_dataset=CustomDataset(test_split.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 10\n",
    "loaders = {\n",
    "    'train': DataLoader(train_split_dataset, batch_size=batch, shuffle=True, num_workers=0),\n",
    "    'test': DataLoader(test_split_dataset, batch_size=batch, shuffle=True, num_workers=0),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM_simple(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dimension,lstm_units,hidden_dimension):\n",
    "        super().__init__()\n",
    "        self.lstm_block=nn.Sequential(\n",
    "            nn.Embedding(vocab_size,embed_dimension),\n",
    "            nn.LSTM(embed_dimension,lstm_units,hidden_dimension,bidirectional=True,batch_first=True),\n",
    "            \n",
    "        )\n",
    "        self.out_layer = nn.Linear(2*hidden_dimension, 3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, title1,title2):\n",
    "        title1=title1.to(device=device)\n",
    "        title2=title2.to(device=device)\n",
    "\n",
    "        title1_lstm_out,(ht_1,ct_1)= self.lstm_block(title1)\n",
    "        title2_lstm_out, (ht_2, ct_2) = self.lstm_block(title2)\n",
    "\n",
    "\n",
    "        out = torch.cat((ht_1[-1, :, :], ht_2[-1, :, :]), dim=1)\n",
    "        # print(ht_1.size(), ht_2.size(), out.size())\n",
    "        out=self.out_layer(out)\n",
    "        # lstm_out, (ht, ct) = self.lstm_layer1(embedded_text)\n",
    "        # ht=torch.cat((ht[-2, :, :], ht[-1, :, :]), dim=1)\n",
    "        # print(ht.size())\n",
    "        # ht=self.full_layer1(ht)\n",
    "        # print(ht.size())\n",
    "        # ht=self.RELU(ht)\n",
    "        # ht = self.full_layer2(ht)\n",
    "        # ht = self.RELU2(ht)\n",
    "        # ht=self.out_layer(ht)\n",
    "        return out\n",
    "\n",
    "        # return self.full_layer1(ht[-1]).to(device=device)\n",
    "\n",
    "\n",
    "LSTM_simple_model = LSTM_simple(49491, 256, 128, 128).to(device=device) # 49491 is vocab len\n",
    "optimizer=optim.SGD(LSTM_simple_model.parameters(),lr=0.01,momentum=0.9)\n",
    "class_weights=class_weights.to(device=device)\n",
    "loss_func=nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_simple(\n",
       "  (lstm_block): Sequential(\n",
       "    (0): Embedding(49491, 256)\n",
       "    (1): LSTM(256, 128, num_layers=128, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (out_layer): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_simple_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the accuracy of the test_split\n",
    "def validation(model):\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for a,b, labels in loaders['test']:\n",
    "            labels = labels.to(device=device)\n",
    "            test_output = model(a,b)\n",
    "            loss = loss_func(test_output, labels)\n",
    "            pred_y = torch.argmax(test_output, 1).data.squeeze()\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "        return accuracy, loss\n",
    "# validation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mh:\\Edu\\IIT\\Spring 2022\\Online Social Network Analysis\\Tweets_Agreement\\src\\siamese_LSTM.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000015?line=51'>52</a>\u001b[0m         validation_loss_list\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(validation_loss))\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000015?line=52'>53</a>\u001b[0m         validation_accuracy_list\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(validation_accuracy))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000015?line=55'>56</a>\u001b[0m train(num_epochs, LSTM_simple_model, loaders)\n",
      "\u001b[1;32mh:\\Edu\\IIT\\Spring 2022\\Online Social Network Analysis\\Tweets_Agreement\\src\\siamese_LSTM.ipynb Cell 14'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(num_epochs, LSTM_simple_model, loaders)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000015?line=37'>38</a>\u001b[0m training_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000015?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m(i\u001b[39m%\u001b[39m\u001b[39m50\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000015?line=39'>40</a>\u001b[0m     val_acc, val_loss \u001b[39m=\u001b[39m validation(LSTM_simple_model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000015?line=40'>41</a>\u001b[0m     validation_accuracy\u001b[39m.\u001b[39mappend(val_acc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000015?line=41'>42</a>\u001b[0m     validation_loss\u001b[39m.\u001b[39mappend(val_loss)\n",
      "\u001b[1;32mh:\\Edu\\IIT\\Spring 2022\\Online Social Network Analysis\\Tweets_Agreement\\src\\siamese_LSTM.ipynb Cell 13'\u001b[0m in \u001b[0;36mvalidation\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000017?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m a,b, labels \u001b[39min\u001b[39;00m loaders[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000017?line=6'>7</a>\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000017?line=7'>8</a>\u001b[0m     test_output \u001b[39m=\u001b[39m model(a,b)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000017?line=8'>9</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_func(test_output, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000017?line=9'>10</a>\u001b[0m     pred_y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(test_output, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\CVAS4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mh:\\Edu\\IIT\\Spring 2022\\Online Social Network Analysis\\Tweets_Agreement\\src\\siamese_LSTM.ipynb Cell 11'\u001b[0m in \u001b[0;36mLSTM_simple.forward\u001b[1;34m(self, title1, title2)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000010?line=14'>15</a>\u001b[0m title2\u001b[39m=\u001b[39mtitle2\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000010?line=16'>17</a>\u001b[0m title1_lstm_out,(ht_1,ct_1)\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm_block(title1)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000010?line=17'>18</a>\u001b[0m title2_lstm_out, (ht_2, ct_2) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm_block(title2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000010?line=20'>21</a>\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((ht_1[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :, :], ht_2[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :, :]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/h%3A/Edu/IIT/Spring%202022/Online%20Social%20Network%20Analysis/Tweets_Agreement/src/siamese_LSTM.ipynb#ch0000010?line=21'>22</a>\u001b[0m \u001b[39m# print(ht_1.size(), ht_2.size(), out.size())\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\CVAS4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\CVAS4\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\CVAS4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\CVAS4\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/rnn.py?line=758'>759</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/rnn.py?line=759'>760</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/rnn.py?line=760'>761</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/rnn.py?line=761'>762</a>\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/rnn.py?line=762'>763</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/rnn.py?line=763'>764</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/CVAS4/lib/site-packages/torch/nn/modules/rnn.py?line=764'>765</a>\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "training_loss_list = []\n",
    "train_accuracy_list = []\n",
    "validation_loss_list = []\n",
    "validation_accuracy_list = []\n",
    "def train(num_epochs, LSTM_simple_model, loaders):\n",
    "    LSTM_simple_model.train()\n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        training_loss = []\n",
    "        train_accuracy = []\n",
    "        validation_loss = []\n",
    "        validation_accuracy = []\n",
    "    \n",
    "        for i, (data1,data2, labels) in enumerate(loaders['train']):\n",
    "            step_acc = 0.0\n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(data1)   # batch x1\n",
    "            b_x2=Variable(data2)\n",
    "            b_y = Variable(labels)   # batch y\n",
    "            b_x = b_x.to(device=device)\n",
    "            b_x2=b_x2.to(device=device)\n",
    "            b_y = b_y.to(device=device)\n",
    "\n",
    "            # clear gradients for this training step\n",
    "            optimizer.zero_grad()\n",
    "            # predicted output from the net\n",
    "            output = LSTM_simple_model(b_x, b_x2)\n",
    "            pred_y = torch.argmax(output, 1).data.squeeze()\n",
    "\n",
    "            step_acc = (pred_y == b_y).sum().item() / float(labels.size(0))\n",
    "            #calc cross entropy loss\n",
    "            loss = loss_func(output, b_y)\n",
    "            train_accuracy.append(step_acc)\n",
    "            training_loss.append(loss.item())\n",
    "            val_acc, val_loss = validation(LSTM_simple_model)\n",
    "            validation_accuracy.append(val_acc)\n",
    "            validation_loss.append(val_loss)\n",
    "            # backpropagation, compute gradients\n",
    "            loss.backward()     # apply gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, acc: {:.4f}' .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), step_acc))\n",
    "        training_loss_list.append(np.mean(training_loss))\n",
    "        train_accuracy_list.append(np.mean(train_accuracy))\n",
    "        validation_loss_list.append(np.mean(validation_loss))\n",
    "        validation_accuracy_list.append(np.mean(validation_accuracy))\n",
    "\n",
    "\n",
    "train(num_epochs, LSTM_simple_model, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(LSTM_simple_model.state_dict(),'./saved_models/lstm_model_siamese.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0c1eaf41ec52b322af8555e1405dc2c6c1d8f2144cde1211d67794db22edf07"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('CVAS4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
