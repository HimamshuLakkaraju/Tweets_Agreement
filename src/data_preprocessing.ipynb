{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as aug_word\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLPAUG SynonymAug function to generate similar words (Below will replace atleast 2 words in a sentence with a max of 4 words replaced in a sentence passed.)\n",
    "similar_word = aug_word.SynonymAug(aug_min=2, aug_max=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Train, Test  data from files\n",
    "train_file = pd.read_csv('../data/train.csv')\n",
    "final_test_file = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows grouped by label are:\n",
      "unrelated class rows: 175598 (68.475)% of total train data\n",
      "agreed class rows: 74238 (28.949)% of total train data\n",
      "disagreed class rows: 6606 (2.576)% of total train data\n"
     ]
    }
   ],
   "source": [
    "# Print % of data available for each class label\n",
    "data_per_class = Counter(train_file['label'])\n",
    "print(\"Number of rows grouped by label are:\")\n",
    "for i, label in enumerate(data_per_class):\n",
    "    print(\n",
    "        f\"{label} class rows: {data_per_class[label]} ({data_per_class[label] / train_file.shape[0]*100:.3f})% of total train data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing step. Converts labels to numbers, converts titles to lowercase sentences without special chars\n",
    "label_encoding = {\"unrelated\":0,\"agreed\":1,\"disagreed\":2}\n",
    "# preprocessing to remove special chars and convert text to lowercase.\n",
    "def preprocessing(txt):\n",
    "    txt = re.sub('[^a-zA-Z0-9 ]', '', txt)\n",
    "    txt = txt.lower()\n",
    "    return txt\n",
    "\n",
    "# convert text labels to numbers\n",
    "def convert_labels(txt):\n",
    "    return label_encoding[txt]\n",
    "\n",
    "\n",
    "train_file['title1_en'] = train_file['title1_en'].apply(preprocessing)\n",
    "train_file['title2_en'] = train_file['title2_en'].apply(preprocessing)\n",
    "train_file['label'] = train_file['label'].apply(convert_labels)\n",
    "\n",
    "final_test_file['title1_en'] = final_test_file['title1_en'].apply(preprocessing)\n",
    "final_test_file['title2_en']= final_test_file['title2_en'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "is bad\n"
     ]
    }
   ],
   "source": [
    "samp='test\\nis bad'\n",
    "print(samp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testis bad\n"
     ]
    }
   ],
   "source": [
    "samp=preprocessing(samp)\n",
    "print(samp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data for agreed and disagreed labels\n",
    "disagreed_df = train_file.loc[train_file['label'] ==2]\n",
    "agreed_df=train_file.loc[train_file['label'] ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation can be used as a data augmentation technique, commented as this takes long time especially with data this size.\n",
    "# translator_en_fr = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "# translator_fr_de = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-de\")\n",
    "# translator_de_es = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-de-es\")\n",
    "# translator_es_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "\n",
    "# def translate(ls):\n",
    "#     en_fr = translator_en_fr(ls)\n",
    "#     trans_en_fr = []\n",
    "#     [trans_en_fr.append(x['translation_text']) for x in en_fr]\n",
    "\n",
    "#     fr_de = translator_fr_de(trans_en_fr)\n",
    "#     trans_fr_de = []\n",
    "#     [trans_fr_de.append(x['translation_text']) for x in fr_de]\n",
    "\n",
    "#     de_es = translator_de_es(trans_fr_de)\n",
    "#     trans_de_es = []\n",
    "#     [trans_de_es.append(x['translation_text']) for x in de_es]\n",
    "\n",
    "#     es_en = translator_es_en(trans_de_es)\n",
    "#     trans_es_en = []\n",
    "#     [trans_es_en.append(x['translation_text'].lower()) for x in es_en]\n",
    "\n",
    "#     return trans_es_en\n",
    "# translated_disagreed_title1=disagreed_df['title1_en'].apply(translate)\n",
    "# translated_disagreed_title2=disagreed_df['title2_en'].apply(translate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation step for disagreed class. Loops through the diagreed class rows and geenrates similar sentences.\n",
    "def augmented_text(txt):\n",
    "    generated_text=similar_word.augment(txt)\n",
    "    return generated_text\n",
    "\n",
    "for i in range(4):\n",
    "    disagreed_data_title1_en=(disagreed_df['title1_en'].apply(augmented_text))\n",
    "    disagreed_data_title2_en=(disagreed_df['title2_en'].apply(augmented_text))\n",
    "    disagreed_data_label=pd.Series([2]*len(disagreed_data_title1_en))\n",
    "    disagreed_data_title1_en.reset_index(drop=True,inplace=True)\n",
    "    disagreed_data_title2_en.reset_index(drop=True, inplace=True)\n",
    "    augmented_disagreed_data_df = pd.DataFrame({\"title1_en\": disagreed_data_title1_en, \"title2_en\": disagreed_data_title2_en,\n",
    "                                            \"label\": disagreed_data_label})\n",
    "    \n",
    "    augmented_disagreed_data_df['title1_en'] = augmented_disagreed_data_df['title1_en'].apply(preprocessing)\n",
    "    augmented_disagreed_data_df['title2_en'] = augmented_disagreed_data_df['title2_en'].apply(preprocessing)\n",
    "    disagreed_df=pd.concat([disagreed_df, augmented_disagreed_data_df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation step for agreed class. Loops through the diagreed class rows and geenrates similar sentences.\n",
    "for i in range(1):\n",
    "    agreed_data_title1_en = (agreed_df['title1_en'].apply(augmented_text))\n",
    "    agreed_data_title2_en = (agreed_df['title2_en'].apply(augmented_text))\n",
    "    agreed_data_label = pd.Series([1]*len(agreed_data_title1_en))\n",
    "    agreed_data_title1_en.reset_index(drop=True, inplace=True)\n",
    "    agreed_data_title2_en.reset_index(drop=True, inplace=True)\n",
    "    augmented_agreed_data_df = pd.DataFrame(\n",
    "        {\"title1_en\": agreed_data_title1_en, \"title2_en\": agreed_data_title2_en, \"label\": agreed_data_label})\n",
    "\n",
    "    augmented_agreed_data_df['title1_en'] = augmented_agreed_data_df['title1_en'].apply(preprocessing)\n",
    "    augmented_agreed_data_df['title2_en'] = augmented_agreed_data_df['title2_en'].apply(preprocessing)\n",
    "    agreed_df = pd.concat([agreed_df, augmented_agreed_data_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrelated_df=train_file.loc[train_file['label'] ==0]\n",
    "disagreed_df_orig=train_file.loc[train_file['label'] ==2]\n",
    "disagreed_df=pd.concat([disagreed_df_orig,disagreed_df],ignore_index=True) # add original data to the augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat final training files.\n",
    "final_train_df = pd.concat([unrelated_df, disagreed_df, agreed_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to vocab and encode data\n",
    "import spacy\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "def tokenize(text):\n",
    "    return [str(token.text) for token in tokenizer.tokenizer(text)]\n",
    "\n",
    "counts = Counter()\n",
    "for index, row in train_file[['title1_en', 'title2_en']].iterrows():\n",
    "    counts.update(tokenize(str(row['title1_en'])+\" \"+str(row['title2_en'])))\n",
    "\n",
    "# Less frequent words can be deleted to decrease vocab size. Not necessary for this dataset\n",
    "# deleted_words={}\n",
    "# print(len(counts.keys()))\n",
    "# for keys in list(counts):\n",
    "#     if counts[keys]<5:\n",
    "#         deleted_words[keys] = counts[keys]\n",
    "#         del counts[keys]\n",
    "# print(len(counts.keys()))\n",
    "# print(deleted_words)\n",
    "\n",
    "vocab2index = {\"\": 0, \"UNK\": 1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)\n",
    "\n",
    "\n",
    "def encode_sentence(text, vocab2index=vocab2index, max_length=30):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(max_length, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(max_length, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    \n",
    "    return encoded, length\n",
    "    \n",
    "def join_encode_text(text):\n",
    "    # print(type(text))\n",
    "   combibed_text = str(text['title1_en'])+\" \"+str(text['title2_en'])\n",
    "   return encode_sentence(combibed_text, vocab2index)[0]\n",
    "\n",
    "\n",
    "final_train_df['encoded_titles_combined'] = final_train_df[['title1_en', 'title2_en']].apply(join_encode_text, axis=1)\n",
    "final_train_df['title1_en_encoded']=final_train_df['title1_en'].apply(encode_sentence)\n",
    "final_train_df['title2_en_encoded']=final_train_df['title2_en'].apply(encode_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows grouped by label after data augmentation is:\n",
      "0 class rows: 175598 (40.240)% of total train data\n",
      "2 class rows: 112302 (25.735)% of total train data\n",
      "1 class rows: 148476 (34.025)% of total train data\n"
     ]
    }
   ],
   "source": [
    "#Checking data distribution by class after data augmentation\n",
    "data_per_class = Counter(final_train_df['label'])\n",
    "print(\"Number of rows grouped by label after data augmentation is:\")\n",
    "for i, label in enumerate(data_per_class):\n",
    "    print(\n",
    "        f\"{label} class rows: {data_per_class[label]} ({data_per_class[label] / final_train_df.shape[0]*100:.3f})% of total train data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train file after data augmentation\n",
    "if (os.path.exists('../data/augmented_training_file/')):\n",
    "    final_train_df.to_csv('../data/augmented_training_file/final_train_file2.csv', index=False, columns=[\n",
    "                          'title1_en', 'title2_en', 'title1_en_encoded', 'title2_en_encoded', 'encoded_titles_combined', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode test file \n",
    "final_test_file['encoded_titles_combined'] = final_test_file[['title1_en', 'title2_en']].apply(join_encode_text, axis=1)\n",
    "final_test_file['title1_en_encoded']=final_test_file['title1_en'].apply(encode_sentence)\n",
    "final_test_file['title2_en_encoded']=final_test_file['title2_en'].apply(encode_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save test file\n",
    "final_test_file.to_csv('../data/final_test_file.csv', index=False, columns=[\n",
    "    'title1_en', 'title2_en', 'title1_en_encoded', 'title2_en_encoded', 'encoded_titles_combined'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab from train file is 49491\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of vocab from train file is {len(vocab2index)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0c1eaf41ec52b322af8555e1405dc2c6c1d8f2144cde1211d67794db22edf07"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('CVAS4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
